{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580519bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "title: \"Basic Concepts of Statistical Machine Learning\"\n",
    "author: \"Fokoue Ernest, School of Mathematical Sciences\"\n",
    "output:\n",
    "  html_document:\n",
    "    df_print: paged\n",
    "  html_notebook: default\n",
    "  pdf_document: default\n",
    "---\n",
    "\n",
    "\n",
    "This segment of the course provides a step by step introduction to some of the  most\n",
    "foundational concepts of statistical machine learning, namely cross validation and the \n",
    "related stochastic hold out for comparing different statistical learning machines. The \n",
    "beautiful graphical tool known as Receiver Operating Characteristic Curve (ROC) is \n",
    "introduced. All these concepts are introduced using k Nearest Neighbors Learning Machines\n",
    "\n",
    "\n",
    "```{R}\n",
    "    library(class)\n",
    "    library(MASS)\n",
    "    library(kernlab)\n",
    "    library(mlbench)\n",
    "    library(reshape2)\n",
    "    library(ROCR)\n",
    "    library(ggplot2)\n",
    "    library(ada)\n",
    "    library(adabag)\n",
    "    library(ipred)\n",
    "    library(survival)\n",
    "    library(rchallenge)\n",
    "    library(PerformanceAnalytics)\n",
    "    library(knitr)\n",
    "    library(acepack)\n",
    "    library(caret)\n",
    "    library(HSAUR2)\n",
    "    library(corrplot)\n",
    "\n",
    "```\n",
    "  \n",
    " \n",
    "## Datasets used in this segment\n",
    "```{r} \n",
    " \n",
    "\n",
    "library(readr)\n",
    "\n",
    "library(dplyr)\n",
    "\n",
    "mnist_train <- read_csv(\"https://pjreddie.com/media/files/mnist_train.csv\", col_names = FALSE)\n",
    "\n",
    "mnist_test <- read_csv(\"https://pjreddie.com/media/files/mnist_test.csv\", col_names = FALSE)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```{r} \n",
    "head(mnist_train)\n",
    "tail(mnist_train)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```{r} \n",
    "str(mnist_train)\n",
    "str(mnist_test)\n",
    "```\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "```{R}\n",
    "  #xy <- read.csv('doughnuts-easy.csv') \n",
    "  #xy <- read.csv('doughnuts.csv') \n",
    "  #xy <- read.csv('four-corners-data-1.csv') \n",
    "  #xy <- read.csv('simple-2d-for-knn-2.csv'); colnames(xy)[2:3]<-c('X1','X2') \n",
    "  #xy <- read.csv('simple-2d-for-knn.csv'); colnames(xy)[2:3]<-c('X1','X2')\n",
    "  #xy <- read.csv('banana-shaped-data-1.csv')\n",
    "    #xy <- read.csv('class-faithful.csv'); colnames(xy)<-c('X1','X2','y'); xy[,3] <- ifelse(xy[,3]==1,1,0); \n",
    "```\n",
    "  Let's have a look at the  observations\n",
    "\n",
    "  \n",
    "  \n",
    " \n",
    "\n",
    "```{r}\n",
    "which(is.na(mnist_train))\n",
    "  which(is.na(mnist_test))\n",
    "\n",
    "```\n",
    " Lets extract data for only 1 and 7\n",
    " \n",
    "```{r}\n",
    "\n",
    "mtrain=mnist_train[mnist_train$X1==1 | mnist_train$X1==7, ]\n",
    "mtest=mnist_test[mnist_test$X1==1 | mnist_test$X1==7, ]\n",
    "head(mtrain)\n",
    "head(mtest)\n",
    "\n",
    "```\n",
    "```{r}\n",
    "p=ncol(mtrain)\n",
    "x_train=mtrain[,-1]\n",
    "y_train=mtrain[,1,drop=TRUE]\n",
    "x_test=mtest[,-1]\n",
    "y_test=mtest[,1,drop=TRUE]\n",
    "sum(is.na(x_test))\n",
    "\n",
    " barplot(prop.table(table(rbind(y_train,y_test))), col=2:3, xlab='digits')\n",
    "\n",
    "```\n",
    "Below, we start by considering the 1 nearest neighbors binary classifier on the test set, and the  corresponding confusion matrix is generated\n",
    "\n",
    "\n",
    "```{r}\n",
    "  k   <- 1\n",
    "                              # True responses in test set\n",
    "  y.te.hat <- knn(x_train, x_test, y_train, k=k) # Predicted responses in test set\n",
    "  \n",
    "  conf.mat.te <- table(y_test, y.te.hat)\n",
    "  conf.mat.te\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{r}\n",
    "  k        <- 1\n",
    "  y.tr.hat <- knn(x_train, x_train, y_train, k=k) # Predicted responses in test set\n",
    "  \n",
    "  conf.mat.tr <- table(y_train, y.tr.hat)\n",
    "  conf.mat.tr\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Below, we start by considering the 7 nearest neighbors binary classifier on the test set, and the  corresponding confusion matrix is generated\n",
    "\n",
    "\n",
    "```{r}\n",
    "  k   <- 7\n",
    "                              # True responses in test set\n",
    "  y.te.hat <- knn(x_train, x_test, y_train, k=k) # Predicted responses in test set\n",
    "  \n",
    "  conf.mat.te <- table(y_test, y.te.hat)\n",
    "  conf.mat.te\n",
    "```\n",
    "```{r}\n",
    "  k        <- 7\n",
    "  y.tr.hat <- knn(x_train, x_train, y_train, k=k) # Predicted responses in test set\n",
    "  \n",
    "  conf.mat.tr <- table(y_train, y.tr.hat)\n",
    "  conf.mat.tr\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Below, we start by considering the 9 nearest neighbors binary classifier on the test set, and the  corresponding confusion matrix is generated\n",
    "\n",
    "\n",
    "```{r}\n",
    "  k   <- 9\n",
    "                              # True responses in test set\n",
    "  y.te.hat <- knn(x_train, x_test, y_train, k=k) # Predicted responses in test set\n",
    "  \n",
    "  conf.mat.te <- table(y_test, y.te.hat)\n",
    "  conf.mat.te\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```{r}\n",
    "  k        <- 9\n",
    "  y.tr.hat <- knn(x_train, x_train, y_train, k=k) # Predicted responses in test set\n",
    "  \n",
    "  conf.mat.tr <- table(y_train, y.tr.hat)\n",
    "  conf.mat.tr\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Predictive performance of the 9-Nearest Neighbors Classifier\n",
    "\n",
    "As you know by now, nearest neighbors learning machines are so-called\n",
    "lazy learners since there is no estimation per se in the traditional sense\n",
    "of storing a model. Instead, all the nearest neighbors machine does is perform predictions.\n",
    "If $\\mathcal{Y}$ represents our output space and $\\mathcal{X}$ represent our input space, and $\\mathcal{V}_k(x)$ represents\n",
    "the set of the $k$ nearest neighbors of $x \\in \\mathcal{X}$ in the provided\n",
    "dataset $\\mathcal{D}_n$, then the predicted class $\\widehat{f}_{\\tt kNN}(x)$ of $x \\in \\mathcal{X}$ is given by\n",
    "$$\n",
    "\\widehat{f}_{\\tt kNN}(x) = \\underset{c \\in \\mathcal{Y}}{{\\tt argmax}}\\Bigg\\{\\frac{1}{k}\\sum_{i=1}^n{{\\bf 1}(x_i \\in \\mathcal{V}_k(x)){\\bf 1}(y_i=c)}\\Bigg\\}\n",
    "$$\n",
    "Below, we start by considering the 9 nearest neighbors binary classifier on the test set, and the  corresponding confusion matrix is generated\n",
    "\n",
    "```{R}\n",
    "  library(class)\n",
    "  k        <- 9\n",
    "  y.te     <- y[id.te]                                 # True responses in test set\n",
    "  y.te.hat <- knn(x[id.tr,], x[id.te,], y[id.tr], k=k) # Predicted responses in test set\n",
    "  \n",
    "  conf.mat.te <- table(y.te, y.te.hat)\n",
    "  conf.mat.te\n",
    "```\n",
    "\n",
    "We now consider a different and very special nearest neighbors learning machines, namely\n",
    "the 1 nearest neighbor or simply nearest neighbor learning machine. This time we first\n",
    "consider only the training set, and generate the corresponding confusion matrix\n",
    "\n",
    "```{R}\n",
    "  k        <- 1\n",
    "  y.tr     <- y[id.tr]                                 # True responses in test set\n",
    "  y.tr.hat <- knn(x[id.tr,], x[id.tr,], y[id.tr], k=k) # Predicted responses in test set\n",
    "  \n",
    "  conf.mat.tr <- table(y.tr, y.tr.hat)\n",
    "  conf.mat.tr\n",
    "```\n",
    "\n",
    "We can see above something quite remarkable in the confusion matrix.\n",
    "\n",
    "## Exercise\n",
    "Explain why the off diagonal have the values observed\n",
    "Generate the 1NN test set confusion matrix\n",
    "\n",
    "Below we set alternative ways of generating the very same quantities encountered \n",
    "earlier. This is provided here just for completeness and one can choose any of the\n",
    "ways to generate the predictive quantities of interest\n",
    "\n",
    "```{R}\n",
    "# Indicator of error in test\n",
    "  \n",
    "  ind.err.te <- ifelse(y.te!=y.te.hat,1,0)      # Random variable tracking error. Indicator\n",
    "  ind.err.te\n",
    "  \n",
    "# Identification of misclassified cases\n",
    "  \n",
    "  id.err     <- which(y.te!=y.te.hat)           # Which obs are misclassified  \n",
    "  id.err\n",
    "  \n",
    "# Confusion matrix\n",
    "  \n",
    "  conf.mat.te <- table(y.te, y.te.hat)   \n",
    "  conf.mat.te\n",
    "  \n",
    "# Percentage of correctly classified (accurary)\n",
    "  \n",
    "  pcc.te <- sum(diag(conf.mat.te))/nte\n",
    "  # pcc.te <- 1-mean(ind.err.te)        # Another way from ind.err.te\n",
    "  # pcc.te <- 1-length(id.err.te)/nte   # Yet another way fr\n",
    "  pcc.te\n",
    "  \n",
    "# Test Error\n",
    "  \n",
    "  err.te <- 1-pcc.te                    # Complement of accurary\n",
    "  err.te\n",
    "```\n",
    "\n",
    "$$\n",
    "\\widehat{R}^{(tr)}(\\widehat{f})= \\frac{1}{|\\mathcal{D}_n^{(tr)}|}\\sum_{i=1}^n{{\\bf 1}(z_i \\in \\mathcal{D}_n^{(tr)}){\\tt loss}(y_i, \\widehat{f}(x_i))}\n",
    "$$\n",
    "The corresponding test error is\n",
    "$$\n",
    "\\widehat{R}^{(te)}(\\widehat{f})= \\frac{1}{|\\mathcal{D}_n^{(te)}|}\\sum_{i=1}^n{{\\bf 1}(z_i \\in \\mathcal{D}_n^{(te)}){\\tt loss}(y_i, \\widehat{f}(x_i))}\n",
    "$$\n",
    "Where we also crucially have\n",
    "$$\n",
    "\\mathcal{D}_n^{(tr)} \\cup \\mathcal{D}_n^{(te)} = \\mathcal{D}_n\n",
    "$$\n",
    "So that\n",
    "$$\n",
    "|\\mathcal{D}_n^{(tr)}| + |\\mathcal{D}_n^{(te)}| = |\\mathcal{D}_n| = n.\n",
    "$$\n",
    "\n",
    "The loss function used in the above errors is simply the famous zero-one loss, namely\n",
    "$$\n",
    " {\\tt loss}(y, f(x)) = {\\tt zeroone}(y, f(x)) = {\\bf 1}(y\\neq f(x))=\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\text{if}\\,\\, y \\neq f(x) \\\\\n",
    "0 & \\text{if}\\,\\, y = f(x)\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "## Receiver Operating Characteristics (ROC) Curves\n",
    "\n",
    "We now turn to the creation of the very appealing graphic summary known\n",
    "as the ROC curve. This is arguably one of the best tools in binary\n",
    "classification as it allows the straightforward and very intuitive graphical\n",
    "way of assessing the predictive performance of a binary classifier. The ROC curve is built\n",
    "from two main ingredients, namely the True Positive Rate (TPR) and the False Positive Rate (FPR). For a given estimated learning machine $\\widehat{f}$, we have\n",
    "\n",
    "$$\n",
    "{\\tt TPR}(\\widehat{f}) = \\Pr(\\widehat{f}(X)= +1 | Y = +1) = \\frac{\\Pr(\\widehat{f}(X)=+1 \\, \\text{and} \\, Y = +1) }{\\Pr(Y = + 1)}\n",
    "$$\n",
    "and\n",
    "\n",
    "$$\n",
    "{\\tt FPR}(\\widehat{f}) = \\Pr(\\widehat{f}(X)= +1 | Y = -1) = \\frac{\\Pr(\\widehat{f}(X)=+1 \\, \\text{and} \\, Y = -1) }{\\Pr(Y = - 1)}\n",
    "$$\n",
    "IN practice we neither have ${\\tt FPR}(\\widehat{f})$ nor ${\\tt TPR}(\\widehat{f})$, but instead their estimator $\\widehat{{\\tt FPR}(\\widehat{f})}$ and $\\widehat{{\\tt TPR}(\\widehat{f})}$ respectively.\n",
    "\n",
    "```{R}\n",
    "#\n",
    "#  ROC Curve: Plotting one single ROC Curve\n",
    "#\n",
    "  library(ROCR)\n",
    "  \n",
    "  y.roc   <- as.factor(y)\n",
    "  \n",
    "  kNN.mod <- class::knn(x[id.tr,], x[id.tr,], y.roc[id.tr], k=3, prob=TRUE)\n",
    "  prob    <- attr(kNN.mod, 'prob')\n",
    "  prob    <- 2*ifelse(kNN.mod == \"0\", 1-prob, prob) - 1\n",
    "  \n",
    "  pred.knn <- prediction(prob, y.roc[id.tr])\n",
    "  perf.knn <- performance(pred.knn, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "  plot(perf.knn, col=2, lwd= 2, lty=2, main=paste('ROC curve for kNN with k=3'))\n",
    "  abline(a=0,b=1)\n",
    "```\n",
    "We see above that the area under the curve (AUC) for the 3NN classifier on the test\n",
    "set is not very satisfying. The task of classifying diabetes appears difficult\n",
    "\n",
    "We now consider the creation of the ROC curves of our binary classifiers, but this time\n",
    "based on the training set\n",
    "\n",
    "```{R}\n",
    "#\n",
    "# Comparative ROC Curves on the training set\n",
    "# \n",
    "  library(ROCR)\n",
    "  \n",
    "  y.roc   <- as.factor(y)\n",
    "  \n",
    "  kNN.mod <- class::knn(x[id.tr,], x[id.tr,], y.roc[id.tr], k=1, prob=TRUE)\n",
    "  prob    <- attr(kNN.mod, 'prob')\n",
    "  prob    <- 2*ifelse(kNN.mod == \"0\", 1-prob, prob) - 1\n",
    "  \n",
    "  pred.1NN <- prediction(prob, y.roc[id.tr])\n",
    "  perf.1NN <- performance(pred.1NN, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "  kNN.mod <- class::knn(x[id.tr,], x[id.tr,], y.roc[id.tr], k=13, prob=TRUE)\n",
    "  prob    <- attr(kNN.mod, 'prob')\n",
    "  prob    <- 2*ifelse(kNN.mod == \"0\", 1-prob, prob) - 1\n",
    "  \n",
    "  pred.13NN <- prediction(prob, y.roc[id.tr])\n",
    "  perf.13NN <- performance(pred.13NN, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "  kNN.mod <- class::knn(x[id.tr,], x[id.tr,], y.roc[id.tr], k=28, prob=TRUE)\n",
    "  prob    <- attr(kNN.mod, 'prob')\n",
    "  prob    <- 2*ifelse(kNN.mod == \"0\", 1-prob, prob) - 1\n",
    "  \n",
    "  pred.28NN <- prediction(prob, y.roc[id.tr])\n",
    "  perf.28NN <- performance(pred.28NN, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "  plot(perf.1NN, col=2, lwd= 2, lty=2, main=paste('Comparative ROC curves in Training'))\n",
    "  plot(perf.13NN, col=3, lwd= 2, lty=3, add=TRUE)\n",
    "  plot(perf.28NN, col=4, lwd= 2, lty=4, add=TRUE)\n",
    "  abline(a=0,b=1)\n",
    "  legend('bottomright', inset=0.05, c('1NN','13NN', '28NN'),  col=2:4, lty=2:4)\n",
    "```\n",
    "\n",
    "The comparatibve ROC curves based on the training set appear to declare the famous 1NN classifier the winner. However, this result is not valid, but the predictive comparisons are only truly meaningful if they are based on the test set.\n",
    "\n",
    "```{R}\n",
    "#\n",
    "# Comparative ROC Curves on the test set\n",
    "# \n",
    "  library(ROCR)\n",
    "  \n",
    "  y.roc   <- as.factor(y)\n",
    "  \n",
    "  kNN.mod <- class::knn(x[id.tr,], x[id.te,], y.roc[id.tr], k=1, prob=TRUE)\n",
    "  prob    <- attr(kNN.mod, 'prob')\n",
    "  prob    <- 2*ifelse(kNN.mod == \"0\", 1-prob, prob) - 1\n",
    "  \n",
    "  pred.1NN <- prediction(prob, y.roc[id.te])\n",
    "  perf.1NN <- performance(pred.1NN, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "  kNN.mod <- class::knn(x[id.tr,], x[id.te,], y.roc[id.tr], k=13, prob=TRUE)\n",
    "  prob    <- attr(kNN.mod, 'prob')\n",
    "  prob    <- 2*ifelse(kNN.mod == \"0\", 1-prob, prob) - 1\n",
    "  \n",
    "  pred.13NN <- prediction(prob, y.roc[id.te])\n",
    "  perf.13NN <- performance(pred.13NN, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "  kNN.mod <- class::knn(x[id.tr,], x[id.te,], y.roc[id.tr], k=28, prob=TRUE)\n",
    "  prob    <- attr(kNN.mod, 'prob')\n",
    "  prob    <- 2*ifelse(kNN.mod == \"0\", 1-prob, prob) - 1\n",
    "  \n",
    "  pred.28NN <- prediction(prob, y.roc[id.te])\n",
    "  perf.28NN <- performance(pred.28NN, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "  plot(perf.1NN, col=2, lwd= 2, lty=2, main=paste('Comparison of Predictive ROC curves'))\n",
    "  plot(perf.13NN, col=3, lwd= 2, lty=3, add=TRUE)\n",
    "  plot(perf.28NN, col=4, lwd= 2, lty=4, add=TRUE)\n",
    "  abline(a=0,b=1)\n",
    "  legend('bottomright', inset=0.05, c('1NN','13NN', '28NN'),  col=2:4, lty=2:4)\n",
    "```\n",
    "\n",
    "We see now from the above comparative ROC curves, that 1NN is no longer the winner. It's apparent superior performance on the training set was most likely due to overfitting. This is an important lesson to learn, namely that one must base predictive comparisons on generalization which is mimic by the test set\n",
    "\n",
    "### Comparing learning machines from different function spaces\n",
    "\n",
    "\n",
    "```{R}\n",
    "#\n",
    "# Comparative ROC Curves on the test set\n",
    "# \n",
    "  library(ROCR)\n",
    "  library(class)\n",
    "  library(MASS)\n",
    "  library(kernlab)\n",
    "  library(rpart)\n",
    "  library(ada)\n",
    "\n",
    "  xy[,pos] <- as.factor(xy[,pos])\n",
    "  \n",
    "  y.roc   <- as.factor(y)\n",
    "  \n",
    "  # 1-Linear Discriminant Analysis\n",
    "  \n",
    "  lda.mod  <- lda(x[id.tr,], y.roc[id.tr])\n",
    "  prob     <- predict(lda.mod, x[id.te,])$posterior[,2]\n",
    "  pred.lda <- prediction(prob, y.roc[id.te])\n",
    "  perf.lda <- performance(pred.lda, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "\n",
    "  # 2-Quadratic Discriminant Analysis\n",
    "  \n",
    "  qda.mod  <- qda(x[id.tr,], y.roc[id.tr])\n",
    "  prob     <- predict(qda.mod, x[id.te,])$posterior[,2]\n",
    "  pred.qda <- prediction(prob, y.roc[id.te])\n",
    "  perf.qda <- performance(pred.qda, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "  \n",
    "  # 3-Logistic Regression\n",
    "  \n",
    "  log.mod  <- glm(as.factor(y)~., data=xy[id.tr, ], family=binomial(link='logit'))\n",
    "  prob     <- predict(log.mod, x[id.te,], type='response')  # Probabilities as prediction\n",
    "  pred.log <- prediction(prob, y.roc[id.te])\n",
    "  perf.log <- performance(pred.log, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "  # 4-Naive Bayes \n",
    "  library(naivebayes)\n",
    "  naive.mod  <- naive_bayes(as.factor(y)~., data=xy[id.tr, ])\n",
    "  prob       <- predict(naive.mod, x[id.te,], type='prob')[,2]  # Probabilities as prediction\n",
    "  pred.naive <- prediction(prob, y.roc[id.te])\n",
    "  perf.naive <- performance(pred.naive, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "  # 5-Nearest Neighbors Learning Machine\n",
    "\n",
    "  kNN.mod   <- class::knn(x[id.tr,], x[id.te,], y.roc[id.tr], k=12, prob=TRUE)\n",
    "  prob      <- attr(kNN.mod, 'prob')\n",
    "  prob      <- 2*ifelse(kNN.mod == \"0\", 1-prob, prob) - 1\n",
    "  pred.kNN <- prediction(prob, y.roc[id.te])\n",
    "  perf.kNN <- performance(pred.kNN, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "  # 6-Support Vector Machines\n",
    "  \n",
    "  svm.mod  <- ksvm(as.factor(y)~., data=xy[id.tr, ], kernel='rbfdot', type='C-svc', prob.model=TRUE)\n",
    "  prob     <- predict(svm.mod, x[id.te, ], type='probabilities')[,2]\n",
    "  pred.svm <- prediction(prob, y.roc[id.te])\n",
    "  perf.svm <- performance(pred.svm, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "  # 7-Classification Trees\n",
    "  \n",
    "  tree.mod  <- rpart(as.factor(y)~., data=xy[id.tr, ])\n",
    "  prob      <- predict(tree.mod, x[id.te, ], type='prob')[,2]\n",
    "  pred.tree <- prediction(prob, y.roc[id.te])\n",
    "  perf.tree <- performance(pred.tree, measure='tpr', x.measure='fpr')\n",
    "\n",
    "\n",
    "  # 8-Random Forest\n",
    "  library(randomForest)\n",
    "  rf.mod    <- randomForest(as.factor(y)~., data=xy[id.tr, ])\n",
    "  prob      <- predict(rf.mod, x[id.te, ], type='prob')[,2]\n",
    "  pred.rf   <- prediction(prob, y.roc[id.te])\n",
    "  perf.rf   <- performance(pred.rf, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "  # 9-Stochastic Adaptive Boosting\n",
    "  \n",
    "  boost.mod   <- ada(as.factor(y)~., data=xy[id.tr, ])\n",
    "  prob        <- predict(boost.mod, x[id.te, ], type='probs')[,2]\n",
    "  pred.boost  <- prediction(prob, y.roc[id.te])\n",
    "  perf.boost  <- performance(pred.boost, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "  # 10-Gaussian Processes\n",
    "  \n",
    "  gp.mod    <- gausspr(as.factor(y)~., data=xy[id.tr, ], type='classification', kernel='rbfdot')\n",
    "  prob      <- predict(gp.mod, x[id.te, ], type='prob')[,2]\n",
    "  pred.gp   <- prediction(prob, y.roc[id.te])\n",
    "  perf.gp   <- performance(pred.gp, measure='tpr', x.measure='fpr')\n",
    "  \n",
    "  plot(perf.lda, col=2, lwd= 2, lty=1, main=paste('Comparison of Predictive ROC curves'))\n",
    "  plot(perf.qda, col=3, lwd= 2, lty=1, add=TRUE)\n",
    "  plot(perf.log, col=4, lwd= 2, lty=1, add=TRUE)\n",
    "  plot(perf.naive, col=5, lwd= 2, lty=1, add=TRUE)\n",
    "  plot(perf.kNN, col=6, lwd= 2, lty=1, add=TRUE)\n",
    "  plot(perf.svm, col=7, lwd= 2, lty=1, add=TRUE)\n",
    "  plot(perf.tree, col=8, lwd= 2, lty=1, add=TRUE)\n",
    "  plot(perf.rf, col=9, lwd= 2, lty=1, add=TRUE)\n",
    "  plot(perf.boost, col=10, lwd= 2, lty=1, add=TRUE)\n",
    "  plot(perf.gp, col=11, lwd= 2, lty=1, add=TRUE)\n",
    "  abline(a=0,b=1)\n",
    "  legend('bottomright', inset=0.05, c('LDA','QDA', 'LOG', 'NB', 'kNN','SVM','TREE', 'RF','BOOST','GP'),  \n",
    "         col=2:11, lty=1)\n",
    "```\n",
    "\n",
    "## Cross Validation for Selection Models within a Function Space\n",
    "\n",
    "Within the implicit function space from which each of the k Nearest Neighbors machines\n",
    "are chosen, it is of great interest to find a strategy or a least a criterion for choosing\n",
    "a member of the space, in this case the size of the neighborhood $k$. \n",
    "It turns out that cross validation does allow us to determine the neighborhood size $k$.       \n",
    "\n",
    "$$\n",
    "{\\tt CV}(\\widehat{f}) = \\frac{1}{V}\\sum_{v=1}^{V}{\\widehat{e}_v(\\widehat{f})}\n",
    "$$\n",
    "where the $v^{th}$ cross validated error $\\widehat{e}_v(\\widehat{f})$, is the error made by $\\widehat{f}$ on the $v^{th}$ chunk of the $\\mathcal{D}_n$, with that left out chunk not having been used in the training of $\\widehat{f}$, ie\n",
    "$$\n",
    "\\widehat{e}_v(\\widehat{f}) = \\frac{1}{|\\mathcal{D}_n^{(v)}|}\\sum_{i=1}^n{\\mathbf{1}(z_i \\in \\mathcal{D}_n^{(v)}){\\tt loss}(y_i,\\widehat{f}^{(-v)}(x_i))}\n",
    "$$\n",
    "where $z_i=(x_i, y_i)$ and $\\widehat{f}^{(-v)}(\\cdot)$ is estimator $\\widehat{f}$ obtained on data without the $v^{th}$ chunk $\\mathcal{D}_n^{(v)}$. \n",
    "\n",
    "```{R}\n",
    "  xtr <- x #x[id.tr,]          # Using the only the training for validation \n",
    "  ytr <- y #y[id.tr]           # We could use the entire sample but that's greedy\n",
    "  \n",
    "  vK <- seq(1, 25, by=1)    # Grid of values of k \n",
    "  nK <- length(vK)          # Number of values of k considered\n",
    "  cv.error <-numeric(nK)    # Vector of cross validation errors for each k\n",
    "  nc <- nrow(xtr)           # Number of observations used for cross validation \n",
    "  c   <- 10                 # Number of folds. We are doing c-fold cross validation\n",
    "  \n",
    "  S   <- sample(sample(nc)) # We randomly shuffle the data before starting CV\n",
    "  m   <- ceiling(nc/c)      # Maximum Number of observations in each fold\n",
    "  \n",
    "  held.out.set <- matrix(0, nrow=c, ncol=m) # Table used to track the evolution\n",
    "  \n",
    "  for(ic in 1:(c-1))\n",
    "  {\n",
    "    held.out.set[ic,] <- S[((ic-1)*m + 1):(ic*m)]\n",
    "  }\n",
    "  held.out.set[c, 1:(nc-(c-1)*m)] <- S[((c-1)*m + 1):nc]  # Handling last chunk just in case n!=mc\n",
    "\n",
    "#  \n",
    "# Running the cross validation itself\n",
    "#  \n",
    "  for(j in 1:nK)\n",
    "  { \n",
    "    for(i in 1:c)\n",
    "    {   \n",
    "      out <-  held.out.set[i,] \n",
    "      yhatc<- knn(xtr[-out,], xtr[out,],ytr[-out],  k=vK[j])\n",
    "      cv.error[j]<-cv.error[j] + (length(out)-sum(diag(table(ytr[out],yhatc))))/length(out)\n",
    "    }\n",
    "    cv.error[j]<-cv.error[j]/c\n",
    "  }\n",
    "\n",
    "#  \n",
    "# Plot the cross validation curve\n",
    "#  \n",
    "  plot(vK, cv.error, xlab='k', ylab=expression(CV[Error](k)), \n",
    "      main='Choice of k in k Nearest Neighbor by m-fold Cross Validation') \n",
    "  lines(vK, cv.error, type='c') \n",
    "```\n",
    "\n",
    "For a nicer looking plot, we resort to the famous ggplot\n",
    "\n",
    "```{R}\n",
    "#\n",
    "#  Nicer plot with ggplot2\n",
    "#\n",
    "  library(ggplot2)\n",
    "\n",
    "  cv <- data.frame(vK, cv.error)\n",
    "  colnames(cv) <- c('k','error')\n",
    "  \n",
    "  ggplot(cv, aes(k,error))+geom_point()+geom_line()+\n",
    "    labs(x='k=size of neighborhood', y=expression(CV[Error](k)))\n",
    "```\n",
    "\n",
    "\n",
    "## Predictive performance using stochastic hold out\n",
    "\n",
    "Once members of various function spaces are chosen using perhaps cross validation as \n",
    "above, it becomes interesting to compare several methods via many copies of their\n",
    "test errors generated randomly via stochastic hold out. We first do it for a single\n",
    "learning machine and then we move on to several different learning machines\n",
    "\n",
    "```{R}\n",
    "# \n",
    "# Extract the optimal k yielded by cross validation \n",
    "#  \n",
    "  k.opt.cv <- 9 #max(which(cv.error==min(cv.error)))\n",
    " \n",
    "##############################################################\n",
    "# Using the optimally tuned k let's estimate the test error  #\n",
    "##############################################################\n",
    "  \n",
    "  set.seed (19671210)          # Set seed for random number generation to be reproducible\n",
    "  \n",
    "  epsilon <- 1/3               # Proportion of observations in the test set\n",
    "  nte     <- round(n*epsilon)  # Number of observations in the test set\n",
    "  ntr     <- n - nte\n",
    "  \n",
    "  #k.opt.cv <- 28\n",
    "  \n",
    "  R <- 100   # Number of replications\n",
    "  test.err <- numeric(R)\n",
    "  \n",
    "  for(r in 1:R)\n",
    "  {\n",
    "    # Split the data\n",
    "    \n",
    "    hold <- stratified.holdout(as.factor(xy[,pos]), 1-epsilon) \n",
    "    id.tr <- hold$idx1\n",
    "    id.te <- hold$idx2\n",
    "    ntr   <- length(id.tr)\n",
    "    nte   <- length(id.te)\n",
    "  \n",
    "    y.te         <- y[id.te]                    # True responses in test set\n",
    "    y.te.hat     <- knn(x[id.tr,], x[id.te,], y[id.tr], k=k.opt.cv) # Predicted responses in test set\n",
    "    ind.err.te   <- ifelse(y.te!=y.te.hat,1,0)    # Random variable tracking error. Indicator\n",
    "    test.err[r]  <- mean(ind.err.te)\n",
    "  }  \n",
    "  \n",
    "  test <- data.frame(test.err)\n",
    "  colnames(test) <- c('error')\n",
    "  \n",
    "  ggplot(test, aes(x='', y=error))+geom_boxplot()+\n",
    "  labs(x='Method', y=expression(hat(R)[te](kNN)))\n",
    "  \n",
    "  ##############################################################\n",
    "  #  Predictively compare different kNN  learning machines     #\n",
    "  ##############################################################\n",
    "```\n",
    "\n",
    "We now consider the actually comparison of various different learning machines in terms\n",
    "of their predictive performances obtained via stochastic hold out. In this case, we are\n",
    "comparing different members of the nearest neighbors paradigm, but in reality, we can\n",
    "and will consider learning machines originating from drastically different paradigms\n",
    "\n",
    "```{R}\n",
    "  set.seed (19671210)          # Set seed for random number generation to be reproducible\n",
    "  \n",
    "  epsilon <- 1/3               # Proportion of observations in the test set\n",
    "  \n",
    "  R <- 50   # Number of replications\n",
    "  test.err <- matrix(0, nrow=R, ncol=11)\n",
    "  \n",
    "  library(class)\n",
    "  \n",
    "  for(r in 1:R)\n",
    "  {\n",
    "    # Split the data\n",
    "    \n",
    "    hold <- stratified.holdout(as.factor(xy[,pos]), 1-epsilon) \n",
    "    id.tr <- hold$idx1\n",
    "    id.te <- hold$idx2\n",
    "    ntr   <- length(id.tr)\n",
    "    nte   <- length(id.te)\n",
    "  \n",
    "    y.te         <- y[id.te]                            # True responses in test set\n",
    "    \n",
    "    # 1-Linear Discriminant Analysis\n",
    "  \n",
    "    lda.mod        <- lda(x[id.tr,], y.roc[id.tr])\n",
    "    y.te.hat       <- predict(lda.mod, x[id.te,])$class\n",
    "    ind.err.te     <- ifelse(y.te!=y.te.hat,1,0)    # Random variable tracking error. Indicator\n",
    "    test.err[r,1]  <- mean(ind.err.te)\n",
    "    \n",
    "    # 2-Quadratic Discriminant Analysis\n",
    "  \n",
    "    qda.mod        <- qda(x[id.tr,], y.roc[id.tr])\n",
    "    y.te.hat       <- predict(qda.mod, x[id.te,])$class\n",
    "    ind.err.te     <- ifelse(y.te!=y.te.hat,1,0)                      # Random variable tracking error. Indicator\n",
    "    test.err[r,2]  <- mean(ind.err.te)\n",
    "    \n",
    "    # 3-Logistic Regression\n",
    "  \n",
    "    log.mod        <- glm(as.factor(y)~., data=xy[id.tr, ], family=binomial(link='logit'))\n",
    "    y.te.hat       <- as.factor(ifelse(predict(log.mod, x[id.te,], type='response')>0.5,1,0))  \n",
    "    ind.err.te     <- ifelse(y.te!=y.te.hat,1,0)                      # Random variable tracking error. Indicator\n",
    "    test.err[r,3]  <- mean(ind.err.te)\n",
    "    \n",
    "    # 4-Naive Bayes \n",
    "  \n",
    "    naive.mod      <- naive_bayes(as.factor(y)~., data=xy[id.tr, ])\n",
    "    y.te.hat       <- predict(naive.mod, x[id.te,], type='class')  \n",
    "    ind.err.te     <- ifelse(y.te!=y.te.hat,1,0)                      # Random variable tracking error. Indicator\n",
    "    test.err[r,4]  <- mean(ind.err.te)\n",
    "    \n",
    "    # 5-Nearest Neighbors Learning Machine\n",
    "\n",
    "    y.te.hat       <- knn(x[id.tr,], x[id.te,], y.roc[id.tr], k=12, prob=TRUE)\n",
    "    ind.err.te     <- ifelse(y.te!=y.te.hat,1,0)  # Random variable tracking error. Indicator\n",
    "    test.err[r,5]  <- mean(ind.err.te)\n",
    "    \n",
    "    # 6-Support Vector Machines\n",
    "  \n",
    "    svm.mod        <- ksvm(as.factor(y)~., data=xy[id.tr, ], kernel='rbfdot', type='C-svc', prob.model=TRUE)\n",
    "    y.te.hat       <- predict(svm.mod, x[id.te, ], type='response')\n",
    "    ind.err.te     <- ifelse(y.te!=y.te.hat,1,0)  # Random variable tracking error. Indicator\n",
    "    test.err[r,6]  <- mean(ind.err.te)\n",
    "  \n",
    "    # 7-Classification Trees\n",
    "  \n",
    "    tree.mod       <- rpart(as.factor(y)~., data=xy[id.tr, ])\n",
    "    y.te.hat       <- predict(tree.mod, x[id.te, ], type='class')\n",
    "    ind.err.te     <- ifelse(y.te!=y.te.hat,1,0)  # Random variable tracking error. Indicator\n",
    "    test.err[r,7]  <- mean(ind.err.te)\n",
    "  \n",
    "    # 8-Random Forest\n",
    "  \n",
    "    rf.mod         <- randomForest(as.factor(y)~., data=xy[id.tr, ])\n",
    "    y.te.hat       <- predict(rf.mod, x[id.te, ], type='response')\n",
    "    ind.err.te     <- ifelse(y.te!=y.te.hat,1,0)   # Random variable tracking error. Indicator\n",
    "    test.err[r,8]  <- mean(ind.err.te)\n",
    "  \n",
    "    # 9-Stochastic Adaptive Boosting\n",
    "  \n",
    "    boost.mod      <- ada(as.factor(y)~., data=xy[id.tr, ])\n",
    "    y.te.hat       <- predict(boost.mod, x[id.te, ], type='vector')\n",
    "    ind.err.te     <- ifelse(y.te!=y.te.hat,1,0)  # Random variable tracking error. Indicator\n",
    "    test.err[r,9]  <- mean(ind.err.te)\n",
    "  \n",
    "    # 10-Gaussian Processes\n",
    "  \n",
    "    gp.mod         <- gausspr(as.factor(y)~., data=xy[id.tr, ], type='classification', kernel='rbfdot')\n",
    "    y.te.hat       <- predict(gp.mod, x[id.te, ], type='response')\n",
    "    ind.err.te     <- ifelse(y.te!=y.te.hat,1,0)  # Random variable tracking error. Indicator\n",
    "    test.err[r,10] <- mean(ind.err.te)\n",
    "\n",
    "    # 11-Bagging\n",
    "  \n",
    "    bagging.mod    <- bagging(as.factor(y)~., data=xy[id.tr, ], mfinal=5)\n",
    "    y.te.hat       <- predict(bagging.mod, x[id.te, ], type='class')\n",
    "    ind.err.te     <- ifelse(y.te!=y.te.hat,1,0)    # Random variable tracking error. Indicator\n",
    "    test.err[r,11] <- mean(ind.err.te)\n",
    "    \n",
    "    # 12-Neural Networks\n",
    "    #library(nnet)\n",
    "    #nnet.mod       <- nnet(as.factor(y)~., data=xy[id.tr, ], size=15)\n",
    "    #y.te.hat       <- as.factor(ifelse(predict(nnet.mod, x[id.te,], type='class')==1,1,0))\n",
    "    #ind.err.te     <- ifelse(y.te!=y.te.hat,1,0)   # Random variable tracking error. Indicator\n",
    "    #test.err[r,12] <- mean(ind.err.te)\n",
    "  }  \n",
    "  \n",
    "  test <- data.frame(test.err)\n",
    "  Method<-c('LDA','QDA', 'LOG', 'NB', 'kNN','SVM','TREE', 'RF','BOOST','GP', 'BAG')\n",
    "  colnames(test) <- Method\n",
    "  boxplot(test)\n",
    "```  \n",
    "\n",
    "\n",
    "We can also explore the accuracy\n",
    "```{R}\n",
    "  accu <- data.frame(1-test.err)\n",
    "  Method<-c('LDA','QDA', 'LOG', 'NB', 'kNN','SVM','TREE', 'RF','BOOST','GP', 'BAG')\n",
    "  colnames(accu) <- Method\n",
    "  boxplot(accu)\n",
    "```  \n",
    "\n",
    "Once again, we also produce a nicer looking version of the same plot using the famous ggplot\n",
    "\n",
    "```{R}\n",
    "  require(reshape2)\n",
    "  ggplot(data = melt(test), aes(x=variable, y=value)) + geom_boxplot(aes(fill=variable))+\n",
    "    labs(x='Method', y=expression(hat(A)[te](hat(f))))+\n",
    "    theme(legend.position=\"none\") \n",
    "```\n",
    "\n",
    "Enjoying ggplot for accuracy\n",
    "\n",
    "```{R}\n",
    "  require(reshape2)\n",
    "  ggplot(data = melt(accu), aes(x=variable, y=value)) + geom_boxplot(aes(fill=variable))+\n",
    "    labs(x='Method', y=expression(hat(P)[te](hat(f))))+\n",
    "    theme(legend.position=\"none\") \n",
    "```\n",
    "\n",
    "\n",
    "## Analysis of Variance \n",
    "\n",
    "From a purely statistical perspective, it often becomes important to perform  the\n",
    "inferential method of analysis of variance to more rigorously and more formally\n",
    "compare the learning machines under consideration\n",
    "\n",
    "```{R}\n",
    "#  \n",
    "# Is the difference between the methods significant\n",
    "#  \n",
    "  aov.method <- aov(value~variable, data=melt(test))\n",
    "  anova(aov.method)\n",
    "  #summary(aov.method)\n",
    "  \n",
    "  TukeyHSD(aov.method, ordered = TRUE)\n",
    "  plot(TukeyHSD(aov.method))\n",
    "```\n",
    "The comparison in this case is trivial.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
